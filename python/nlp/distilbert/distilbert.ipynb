{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize DistilBERT with ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample, we optimize the HuggingFace DistilBERT model and measure its performance against PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "If you have Jupyter Notebook, you may directly run this notebook. We will use pip to install or upgrade [PyTorch](https://pytorch.org/), [OnnxRuntime](https://microsoft.github.io/onnxruntime/) and other required packages.\n",
    "\n",
    "Otherwise, you can setup a new environment. First, we install [AnaConda](https://www.anaconda.com/distribution/). Then open an AnaConda prompt window and run the following commands:\n",
    "\n",
    "```console\n",
    "conda create -n cpu_env python=3.6\n",
    "conda activate cpu_env\n",
    "conda install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "The last command will launch Jupyter Notebook and we can open this notebook in browser to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch) (4.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\nakersha\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: onnx in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.10.2)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from onnxruntime) (2.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from onnxruntime) (1.22.2)\n",
      "Requirement already satisfied: protobuf in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from onnxruntime) (3.19.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from onnx) (4.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from onnx) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\nakersha\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.16.2)\n",
      "Requirement already satisfied: requests in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nakersha\\appdata\\roaming\\python\\python38\\site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: click in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: six in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sacremoses->transformers) (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\nakersha\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in c:\\users\\nakersha\\appdata\\roaming\\python\\python38\\site-packages (5.9.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2022.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (8.0.0)\n",
      "Requirement already satisfied: py3nvml in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (1.22.2)\n",
      "Requirement already satisfied: xmltodict in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from py3nvml) (0.12.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\nakersha\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.2)\n",
      "Requirement already satisfied: netron in c:\\users\\nakersha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (5.6.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\nakersha\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "run_install = True # Only need install once\n",
    "if run_install:\n",
    "    if sys.platform in ['linux', 'win32']: # Linux or Windows\n",
    "        !{sys.executable} -m pip install --upgrade torch\n",
    "    else: # Mac\n",
    "        !{sys.executable} -m pip install torch==1.9.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "    !{sys.executable} -m pip install onnxruntime onnx\n",
    "\n",
    "    # Install other packages used in this notebook.\n",
    "    !{sys.executable} -m pip install transformers\n",
    "    !{sys.executable} -m pip install psutil pytz pandas py-cpuinfo py3nvml\n",
    "    !{sys.executable} -m pip install wget netron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DistilBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 5.36kB/s]\n",
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 1.02MB/s]\n",
      "Downloading: 100%|██████████| 426k/426k [00:00<00:00, 1.29MB/s]\n",
      "Downloading: 100%|██████████| 473/473 [00:00<00:00, 94.6kB/s]\n",
      "Downloading: 100%|██████████| 249M/249M [00:18<00:00, 14.4MB/s] \n",
      "Some weights of the model checkpoint at distilbert-base-cased-distilled-squad were not used when initializing DistilBertModel: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n",
    "\n",
    "# The DistilBERT tokenizer is the same as the BERT one\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "\n",
    "# Initializing a model from the configuration\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start downloading predict file.\n",
      "Predict file downloaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cache_dir = os.path.join(\"..\", \"cache_models\")\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "predict_file_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
    "predict_file = os.path.join(cache_dir, \"dev-v1.1.json\")\n",
    "if not os.path.exists(predict_file):\n",
    "    import wget\n",
    "    print(\"Start downloading predict file.\")\n",
    "    wget.download(predict_file_url, predict_file)\n",
    "    print(\"Predict file downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.81it/s]\n",
      "convert squad examples to features: 100%|██████████| 128/128 [00:01<00:00, 121.33it/s]\n",
      "add example index and unique id: 100%|██████████| 128/128 [00:00<00:00, 127522.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers.data.processors.squad import SquadV1Processor\n",
    "\n",
    "processor = SquadV1Processor()\n",
    "examples = processor.get_dev_examples(None, filename=predict_file)\n",
    "\n",
    "total_samples = 128\n",
    "max_seq_length = 128\n",
    "doc_stride = 12\n",
    "max_query_length = 64\n",
    "\n",
    "from transformers import squad_convert_examples_to_features\n",
    "features, dataset = squad_convert_examples_to_features( \n",
    "            examples=examples[:total_samples],\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=False,\n",
    "            return_dataset='pt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the DistilBERT model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(\"..\", \"onnx_models\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)   \n",
    "export_model_path = os.path.join(output_dir, 'distilbert-base-cased-squad.onnx')\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Get the first example data to run the model and export it to ONNX\n",
    "data = dataset[0]\n",
    "inputs = {\n",
    "    'input_ids':      data[0].to(device).reshape(1, max_seq_length),\n",
    "    'input_mask': data[1].to(device).reshape(1, max_seq_length)\n",
    "}\n",
    "\n",
    "# Set model to inference mode, which is required before exporting the model because some operators behave differently in \n",
    "# inference and training mode.\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "    torch.onnx.export(model,                                            # model being run\n",
    "                      tuple(inputs.values()),                           # model input (or a tuple for multiple inputs)\n",
    "                      export_model_path,                                # where to save the model (can be a file or file-like object)\n",
    "                      opset_version=11,                                 # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,                         # whether to execute constant folding for optimization\n",
    "                      input_names=['input_ids',                         # the model's input names\n",
    "                                   'input_mask'], \n",
    "                      output_names=['output'],                    # the model's output names\n",
    "                      dynamic_axes={'input_ids': symbolic_names,        # variable length axes\n",
    "                                    'input_mask' : symbolic_names,\n",
    "                                    'output' : symbolic_names})\n",
    "    print(\"Model exported at \", export_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Inference ##\n",
    "Use PyTorch to evaluate an example input for comparison purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch cpu Inference time = 107.01 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Measure the latency. It is not accurate using Jupyter Notebook, it is recommended to use standalone python script.\n",
    "latency = []\n",
    "with torch.no_grad():\n",
    "    for i in range(total_samples):\n",
    "        data = dataset[i]\n",
    "        inputs = {\n",
    "            'input_ids':      data[0].to(device).reshape(1, max_seq_length),\n",
    "            'input_mask':     data[1].to(device).reshape(1, max_seq_length)\n",
    "        }\n",
    "        start = time.time()\n",
    "        outputs = model(**inputs)\n",
    "        latency.append(time.time() - start)\n",
    "print(\"PyTorch {} Inference time = {} ms\".format(device.type, format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference ONNX Model with ONNX Runtime\n",
    "\n",
    "For Onnx Runtime 1.6.0 or older, OpenMP environment variables are very important for CPU inference of Bert model. Since 1.7.0, the official package is not built with OpenMP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we inference the model with ONNX Runtime. Here we can see that OnnxRuntime has better performance than PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid Feed Input Name:input_mask",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nakersha\\Develop\\code\\microsoft\\onnxruntime-inference-examples\\python\\nlp\\distilbert\\distilbert.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nakersha/Develop/code/microsoft/onnxruntime-inference-examples/python/nlp/distilbert/distilbert.ipynb#ch0000018?line=19'>20</a>\u001b[0m     ort_inputs \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nakersha/Develop/code/microsoft/onnxruntime-inference-examples/python/nlp/distilbert/distilbert.ipynb#ch0000018?line=20'>21</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m:  data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, max_seq_length)\u001b[39m.\u001b[39mnumpy(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nakersha/Develop/code/microsoft/onnxruntime-inference-examples/python/nlp/distilbert/distilbert.ipynb#ch0000018?line=21'>22</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput_mask\u001b[39m\u001b[39m'\u001b[39m: data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, max_seq_length)\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nakersha/Develop/code/microsoft/onnxruntime-inference-examples/python/nlp/distilbert/distilbert.ipynb#ch0000018?line=22'>23</a>\u001b[0m     }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nakersha/Develop/code/microsoft/onnxruntime-inference-examples/python/nlp/distilbert/distilbert.ipynb#ch0000018?line=23'>24</a>\u001b[0m     start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nakersha/Develop/code/microsoft/onnxruntime-inference-examples/python/nlp/distilbert/distilbert.ipynb#ch0000018?line=24'>25</a>\u001b[0m     ort_outputs \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrun(\u001b[39mNone\u001b[39;49;00m, ort_inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nakersha/Develop/code/microsoft/onnxruntime-inference-examples/python/nlp/distilbert/distilbert.ipynb#ch0000018?line=25'>26</a>\u001b[0m     latency\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nakersha/Develop/code/microsoft/onnxruntime-inference-examples/python/nlp/distilbert/distilbert.ipynb#ch0000018?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOnnxRuntime cpu Inference time = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m ms\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mformat\u001b[39m(\u001b[39msum\u001b[39m(latency) \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(latency), \u001b[39m'\u001b[39m\u001b[39m.2f\u001b[39m\u001b[39m'\u001b[39m)))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:192\u001b[0m, in \u001b[0;36mSession.run\u001b[1;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/nakersha/AppData/Local/Programs/Python/Python38/lib/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py?line=189'>190</a>\u001b[0m     output_names \u001b[39m=\u001b[39m [output\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_meta]\n\u001b[0;32m    <a href='file:///c%3A/Users/nakersha/AppData/Local/Programs/Python/Python38/lib/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py?line=190'>191</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/nakersha/AppData/Local/Programs/Python/Python38/lib/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py?line=191'>192</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sess\u001b[39m.\u001b[39;49mrun(output_names, input_feed, run_options)\n\u001b[0;32m    <a href='file:///c%3A/Users/nakersha/AppData/Local/Programs/Python/Python38/lib/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py?line=192'>193</a>\u001b[0m \u001b[39mexcept\u001b[39;00m C\u001b[39m.\u001b[39mEPFail \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    <a href='file:///c%3A/Users/nakersha/AppData/Local/Programs/Python/Python38/lib/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py?line=193'>194</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_fallback:\n",
      "\u001b[1;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid Feed Input Name:input_mask"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Optional: store the optimized graph and view it using Netron to verify that model is fully optimized.\n",
    "# Note that this will increase session creation time, so it is for debugging only.\n",
    "sess_options.optimized_model_filepath = os.path.join(output_dir, \"distilbert-base-cased-squad.onnx\")\n",
    "\n",
    "# For OnnxRuntime 1.7.0 or later, you can set intra_op_num_threads to set thread number like\n",
    "#    sess_options.intra_op_num_threads=4\n",
    "# Here we use the default value which is a good choice in most cases.\n",
    "\n",
    "# Specify providers when you use onnxruntime-gpu for CPU inference.\n",
    "session = onnxruntime.InferenceSession(export_model_path, sess_options, providers=['CPUExecutionProvider'])\n",
    "\n",
    "latency = []\n",
    "for i in range(total_samples):\n",
    "    data = dataset[i]\n",
    "    ort_inputs = {\n",
    "        'input_ids':  data[0].cpu().reshape(1, max_seq_length).numpy(),\n",
    "        'input_mask': data[1].cpu().reshape(1, max_seq_length).numpy()\n",
    "    }\n",
    "    start = time.time()\n",
    "    ort_outputs = session.run(None, ort_inputs)\n",
    "    latency.append(time.time() - start)\n",
    "print(\"OnnxRuntime cpu Inference time = {} ms\".format(format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Verifying correctness *****\n",
      "PyTorch and ONNX Runtime output 0 are close: True\n",
      "PyTorch and ONNX Runtime output 1 are close: True\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Verifying correctness *****\")\n",
    "for i in range(2):\n",
    "    print('PyTorch and ONNX Runtime output {} are close:'.format(i), numpy.allclose(ort_outputs[i], outputs[i].cpu(), rtol=1e-05, atol=1e-04))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Offline Optimization Script and Test Tools\n",
    "\n",
    "It is recommended to try [OnnxRuntime Transformer Model Optimization Tool](https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/transformers) on the exported ONNX models. It could help verify whether the model can be fully optimized, and get performance test results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Optimizer\n",
    "\n",
    "Although OnnxRuntime could optimize Bert model exported by PyTorch. Sometime, model cannot be fully optimized due to different reasons:\n",
    "* A new subgraph pattern is generated by new version of export tool, and the pattern is not covered by older version of OnnxRuntime. \n",
    "* The exported model uses dynamic axis and this makes it harder for shape inference of the graph. That blocks some optimization to be applied.\n",
    "* Some optimization is better to be done offline. Like change input tensor type from int64 to int32 to avoid extra Cast nodes, or convert model to float16 to achieve better performance in V100 or T4 GPU.\n",
    "\n",
    "We have python script **optimizer.py**, which is more flexible in graph pattern matching and model conversion (like float32 to float16). You can also use it to verify whether a Bert model is fully optimized.\n",
    "\n",
    "In this example, we can see that it introduces optimization that is not provided by onnxruntime: SkipLayerNormalization and bias fusion, which is not fused in OnnxRuntime due to shape inference as mentioned.\n",
    "\n",
    "It will also tell whether the model is fully optimized or not. If not, that means you might need change the script to fuse some new pattern of subgraph.\n",
    "\n",
    "Example Usage:\n",
    "```\n",
    "from onnxruntime.transformers import optimizer\n",
    "optimized_model = optimizer.optimize_model(export_model_path, model_type='bert', num_heads=12, hidden_size=768)\n",
    "optimized_model.save_model_to_file(optimized_model_path)\n",
    "```\n",
    "\n",
    "You can also use command line like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               apply: Fused LayerNormalization count: 25\n",
      "               apply: Fused Gelu count: 12\n",
      "adjust_reshape_and_expand: Removed Reshape and Expand count: 0\n",
      "               apply: Fused SkipLayerNormalization count: 24\n",
      "               apply: Fused Attention count: 12\n",
      "         prune_graph: Graph pruned: 0 inputs, 0 outputs and 5 nodes are removed\n",
      "               apply: Fused EmbedLayerNormalization(with mask) count: 1\n",
      "         prune_graph: Graph pruned: 0 inputs, 0 outputs and 3 nodes are removed\n",
      "         prune_graph: Graph pruned: 0 inputs, 0 outputs and 0 nodes are removed\n",
      "               apply: Fused BiasGelu count: 12\n",
      "               apply: Fused SkipLayerNormalization(add bias) count: 24\n",
      "            optimize: opset verion: 11\n",
      "  save_model_to_file: Sort graphs in topological order\n",
      "  save_model_to_file: Output model to ..\\onnx_models\\bert-base-cased-squad_opt_cpu.onnx\n",
      "get_fused_operator_statistics: Optimized operators:{'EmbedLayerNormalization': 1, 'Attention': 12, 'Gelu': 0, 'FastGelu': 0, 'BiasGelu': 12, 'LayerNormalization': 0, 'SkipLayerNormalization': 24}\n",
      "                main: The model has been fully optimized.\n"
     ]
    }
   ],
   "source": [
    "optimized_model_path = os.path.join(output_dir, 'bert-base-cased-squad_opt_cpu.onnx')\n",
    "\n",
    "!{sys.executable} -m onnxruntime.transformers.optimizer --input $export_model_path --output $optimized_model_path --model_type bert --num_heads 12 --hidden_size 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimized Graph\n",
    "When you can open the optimized model using Netron to visualize, the graph is like the following:\n",
    "<img src='images/optimized_bert_gpu.png'>\n",
    "\n",
    "For CPU, optimized graph is slightly different: FastGelu is replaced by BiasGelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '..\\onnx_models\\bert-base-cased-squad_opt_cpu.onnx' at http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "import netron\n",
    "\n",
    "# Change it to False to skip viewing the optimized model in browser.\n",
    "enable_netron = True\n",
    "if enable_netron:\n",
    "    # If you encounter error \"access a socket in a way forbidden by its access permissions\", install Netron as standalone application instead.\n",
    "    netron.start(optimized_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Results Comparison Tool\n",
    "\n",
    "If your BERT model has three inputs, a script compare_bert_results.py can be used to do a quick verification. The tool will generate some fake input data, and compare results from both the original and optimized models. If outputs are all close, it is safe to use the optimized model.\n",
    "\n",
    "Example of verifying models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% passed for 100 random inputs given thresholds (rtol=0.001, atol=0.0001).\n",
      "maximum absolute difference=4.604458808898926e-06\n",
      "maximum relative difference=0.006278202868998051\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m onnxruntime.transformers.compare_bert_results --baseline_model $export_model_path --optimized_model $optimized_model_path --batch_size 1 --sequence_length 128 --samples 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Test Tool\n",
    "\n",
    "This tool measures performance of BERT model inference using OnnxRuntime Python API.\n",
    "\n",
    "The following command will create 100 samples of batch_size 1 and sequence length 128 to run inference, then calculate performance numbers like average latency and throughput etc. You can increase number of samples (recommended 1000) to get more stable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test: model=bert-base-cased-squad_opt_cpu.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=12,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=False\n",
      "Average latency = 54.26 ms, Throughput = 18.43 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_cpu.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=11,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=False\n",
      "Average latency = 55.80 ms, Throughput = 17.92 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_cpu.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=10,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=False\n",
      "Average latency = 65.31 ms, Throughput = 15.31 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_cpu.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=9,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=False\n",
      "Average latency = 57.66 ms, Throughput = 17.34 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_cpu.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=8,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=False\n",
      "Average latency = 62.84 ms, Throughput = 15.91 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_cpu.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=7,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=False\n",
      "Average latency = 69.29 ms, Throughput = 14.43 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_cpu.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=6,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=False\n",
      "Average latency = 56.19 ms, Throughput = 17.80 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_cpu.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=5,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=False\n",
      "Average latency = 59.90 ms, Throughput = 16.70 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_cpu.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=4,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=False\n",
      "Average latency = 63.72 ms, Throughput = 15.69 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_cpu.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=3,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=False\n",
      "Average latency = 82.44 ms, Throughput = 12.13 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_cpu.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=2,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=False\n",
      "Average latency = 119.64 ms, Throughput = 8.36 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_cpu.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=1,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=False\n",
      "Average latency = 223.21 ms, Throughput = 4.48 QPS\n",
      "test setting TestSetting(batch_size=1, sequence_length=128, test_cases=100, test_times=1, use_gpu=False, intra_op_num_threads=None, seed=3, verbose=False)\n",
      "Generating 100 samples for batch_size=1 sequence_length=128\n",
      "Test summary is saved to ..\\onnx_models\\perf_results_CPU_B1_S128_20210713-144140.txt\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m onnxruntime.transformers.bert_perf_test --model $optimized_model_path --batch_size 1 --sequence_length 128 --samples 100 --test_times 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the summary file and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\onnx_models\\perf_results_CPU_B1_S128_20210713-144140.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latency(ms)</th>\n",
       "      <th>Latency_P75</th>\n",
       "      <th>Latency_P90</th>\n",
       "      <th>Latency_P99</th>\n",
       "      <th>Throughput(QPS)</th>\n",
       "      <th>intra_op_num_threads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54.26</td>\n",
       "      <td>56.05</td>\n",
       "      <td>60.32</td>\n",
       "      <td>109.21</td>\n",
       "      <td>18.43</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.80</td>\n",
       "      <td>56.74</td>\n",
       "      <td>59.67</td>\n",
       "      <td>73.62</td>\n",
       "      <td>17.92</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56.19</td>\n",
       "      <td>61.29</td>\n",
       "      <td>71.69</td>\n",
       "      <td>80.15</td>\n",
       "      <td>17.80</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57.66</td>\n",
       "      <td>58.50</td>\n",
       "      <td>61.96</td>\n",
       "      <td>65.12</td>\n",
       "      <td>17.34</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.90</td>\n",
       "      <td>59.72</td>\n",
       "      <td>65.16</td>\n",
       "      <td>116.16</td>\n",
       "      <td>16.70</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>62.84</td>\n",
       "      <td>67.05</td>\n",
       "      <td>69.07</td>\n",
       "      <td>75.99</td>\n",
       "      <td>15.91</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>63.72</td>\n",
       "      <td>64.17</td>\n",
       "      <td>69.44</td>\n",
       "      <td>73.10</td>\n",
       "      <td>15.69</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>65.31</td>\n",
       "      <td>65.35</td>\n",
       "      <td>80.70</td>\n",
       "      <td>177.94</td>\n",
       "      <td>15.31</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>69.29</td>\n",
       "      <td>69.04</td>\n",
       "      <td>70.68</td>\n",
       "      <td>85.03</td>\n",
       "      <td>14.43</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>82.44</td>\n",
       "      <td>83.20</td>\n",
       "      <td>89.64</td>\n",
       "      <td>98.80</td>\n",
       "      <td>12.13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>119.64</td>\n",
       "      <td>119.07</td>\n",
       "      <td>122.62</td>\n",
       "      <td>135.67</td>\n",
       "      <td>8.36</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>223.21</td>\n",
       "      <td>223.22</td>\n",
       "      <td>226.83</td>\n",
       "      <td>249.08</td>\n",
       "      <td>4.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Latency(ms)  Latency_P75  Latency_P90  Latency_P99  Throughput(QPS)  \\\n",
       "0         54.26        56.05        60.32       109.21            18.43   \n",
       "1         55.80        56.74        59.67        73.62            17.92   \n",
       "2         56.19        61.29        71.69        80.15            17.80   \n",
       "3         57.66        58.50        61.96        65.12            17.34   \n",
       "4         59.90        59.72        65.16       116.16            16.70   \n",
       "5         62.84        67.05        69.07        75.99            15.91   \n",
       "6         63.72        64.17        69.44        73.10            15.69   \n",
       "7         65.31        65.35        80.70       177.94            15.31   \n",
       "8         69.29        69.04        70.68        85.03            14.43   \n",
       "9         82.44        83.20        89.64        98.80            12.13   \n",
       "10       119.64       119.07       122.62       135.67             8.36   \n",
       "11       223.21       223.22       226.83       249.08             4.48   \n",
       "\n",
       "    intra_op_num_threads  \n",
       "0                     12  \n",
       "1                     11  \n",
       "2                      6  \n",
       "3                      9  \n",
       "4                      5  \n",
       "5                      8  \n",
       "6                      4  \n",
       "7                     10  \n",
       "8                      7  \n",
       "9                      3  \n",
       "10                     2  \n",
       "11                     1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob     \n",
    "import pandas\n",
    "latest_result_file = max(glob.glob(os.path.join(output_dir, \"perf_results_*.txt\")), key=os.path.getmtime)\n",
    "result_data = pandas.read_table(latest_result_file, converters={'OMP_NUM_THREADS': str, 'OMP_WAIT_POLICY':str})\n",
    "print(latest_result_file)\n",
    "\n",
    "# Remove some columns that have same values for all rows.\n",
    "columns_to_remove = ['model', 'graph_optimization_level', 'batch_size', 'sequence_length', 'test_cases', 'test_times', 'use_gpu']\n",
    "# Hide some latency percentile columns to fit screen width.\n",
    "columns_to_remove.extend(['Latency_P50', 'Latency_P95'])\n",
    "result_data.drop(columns_to_remove, axis=1, inplace=True)\n",
    "result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Additional Info\n",
    "\n",
    "Note that running Jupyter Notebook has slight impact on performance result since Jupyter Notebook is using system resources like CPU and memory etc. It is recommended to close Jupyter Notebook and other applications, then run the performance test tool in a console to get more accurate performance numbers.\n",
    "\n",
    "We have a [benchmark script](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/run_benchmark.sh). It is recommended to use it compare inference speed of OnnxRuntime with PyTorch.\n",
    "\n",
    "[OnnxRuntime C API](https://github.com/microsoft/onnxruntime/blob/master/docs/C_API.md) could get slightly better performance than python API. If you use C API in inference, you can use OnnxRuntime_Perf_Test.exe built from source to measure performance instead.\n",
    "\n",
    "Here is the machine configuration that generated the above results. The machine has GPU but not used in CPU inference.\n",
    "You might get slower or faster result based on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gpu\": {\n",
      "    \"driver_version\": \"470.14\",\n",
      "    \"devices\": [\n",
      "      {\n",
      "        \"memory_total\": 8589934592,\n",
      "        \"memory_available\": 6782619648,\n",
      "        \"name\": \"NVIDIA GeForce GTX 1070\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"cpu\": {\n",
      "    \"brand\": \"Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz\",\n",
      "    \"cores\": 6,\n",
      "    \"logical_cores\": 12,\n",
      "    \"hz\": \"3.1920 GHz\",\n",
      "    \"l2_cache\": \"1536 KB\",\n",
      "    \"flags\": [\n",
      "      \"3dnow\",\n",
      "      \"3dnowprefetch\",\n",
      "      \"abm\",\n",
      "      \"acpi\",\n",
      "      \"adx\",\n",
      "      \"aes\",\n",
      "      \"apic\",\n",
      "      \"avx\",\n",
      "      \"avx2\",\n",
      "      \"bmi1\",\n",
      "      \"bmi2\",\n",
      "      \"clflush\",\n",
      "      \"clflushopt\",\n",
      "      \"cmov\",\n",
      "      \"cx16\",\n",
      "      \"cx8\",\n",
      "      \"de\",\n",
      "      \"dtes64\",\n",
      "      \"dts\",\n",
      "      \"erms\",\n",
      "      \"est\",\n",
      "      \"f16c\",\n",
      "      \"fma\",\n",
      "      \"fpu\",\n",
      "      \"fxsr\",\n",
      "      \"hle\",\n",
      "      \"ht\",\n",
      "      \"hypervisor\",\n",
      "      \"ia64\",\n",
      "      \"invpcid\",\n",
      "      \"lahf_lm\",\n",
      "      \"mca\",\n",
      "      \"mce\",\n",
      "      \"mmx\",\n",
      "      \"movbe\",\n",
      "      \"mpx\",\n",
      "      \"msr\",\n",
      "      \"mtrr\",\n",
      "      \"osxsave\",\n",
      "      \"pae\",\n",
      "      \"pat\",\n",
      "      \"pbe\",\n",
      "      \"pcid\",\n",
      "      \"pclmulqdq\",\n",
      "      \"pdcm\",\n",
      "      \"pge\",\n",
      "      \"pni\",\n",
      "      \"popcnt\",\n",
      "      \"pse\",\n",
      "      \"pse36\",\n",
      "      \"rdrnd\",\n",
      "      \"rdseed\",\n",
      "      \"rtm\",\n",
      "      \"sep\",\n",
      "      \"serial\",\n",
      "      \"sgx\",\n",
      "      \"sgx_lc\",\n",
      "      \"smap\",\n",
      "      \"smep\",\n",
      "      \"ss\",\n",
      "      \"sse\",\n",
      "      \"sse2\",\n",
      "      \"sse4_1\",\n",
      "      \"sse4_2\",\n",
      "      \"ssse3\",\n",
      "      \"tm\",\n",
      "      \"tm2\",\n",
      "      \"tsc\",\n",
      "      \"tscdeadline\",\n",
      "      \"vme\",\n",
      "      \"x2apic\",\n",
      "      \"xsave\",\n",
      "      \"xtpr\"\n",
      "    ],\n",
      "    \"processor\": \"Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\"\n",
      "  },\n",
      "  \"memory\": {\n",
      "    \"total\": 16977195008,\n",
      "    \"available\": 6085459968\n",
      "  },\n",
      "  \"python\": \"3.6.10.final.0 (64 bit)\",\n",
      "  \"os\": \"Windows-10-10.0.21390-SP0\",\n",
      "  \"onnxruntime\": {\n",
      "    \"version\": \"1.8.1\",\n",
      "    \"support_gpu\": false\n",
      "  },\n",
      "  \"onnxruntime_tools\": null,\n",
      "  \"pytorch\": {\n",
      "    \"version\": \"1.9.0+cpu\",\n",
      "    \"support_gpu\": false,\n",
      "    \"cuda\": null\n",
      "  },\n",
      "  \"tensorflow\": {\n",
      "    \"version\": \"2.3.0\",\n",
      "    \"git_version\": \"v2.3.0-rc2-23-gb36436b087\",\n",
      "    \"support_gpu\": true\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-13 14:41:45.376756: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2021-07-13 14:41:45.376780: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m onnxruntime.transformers.machine_info --silent"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f0528bd5f6ac7ac52612a9005b50b1458e2b7d1852103f3c7d5ad6e0d876b262"
  },
  "kernelspec": {
   "display_name": "cpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
